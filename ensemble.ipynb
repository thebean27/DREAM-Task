{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74888880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMich train shape : (500, 138)\n",
      "Belfact train shape : (500, 14221)\n",
      "\n",
      "computing OOF predictions\n",
      "fold  1 done\n",
      "fold  2 done\n",
      "fold  3 done\n",
      "fold  4 done\n",
      "fold  5 done\n",
      "fold  6 done\n",
      "fold  7 done\n",
      "fold  8 done\n",
      "fold  9 done\n",
      "fold 10 done\n",
      "\n",
      "–– Simple ½-½ blend ––\n",
      "Pearson = 0.6455245596903486 |  RMSE 0.120729625\n",
      "\n",
      "searching best scalar weight …\n",
      "w=0.0 | Pearson 0.5659977933592109 | RMSE 0.12912186980247498\n",
      "w=0.02 | Pearson 0.5702159618705038 | RMSE 0.12869641184806824\n",
      "w=0.04 | Pearson 0.5743637898065502 | RMSE 0.1282779425382614\n",
      "w=0.06 | Pearson 0.5784386866430715 | RMSE 0.1278664916753769\n",
      "w=0.08 | Pearson 0.5824380328770289 | RMSE 0.12746216356754303\n",
      "w=0.1 | Pearson 0.5863593677120997 | RMSE 0.1270650178194046\n",
      "w=0.12 | Pearson 0.5902002742963645 | RMSE 0.12667511403560638\n",
      "w=0.14 | Pearson 0.5939584573462474 | RMSE 0.12629252672195435\n",
      "w=0.16 | Pearson 0.5976316694369294 | RMSE 0.12591731548309326\n",
      "w=0.18 | Pearson 0.601217831031718 | RMSE 0.1255495548248291\n",
      "w=0.2 | Pearson 0.6047149522324333 | RMSE 0.12518930435180664\n",
      "w=0.22 | Pearson 0.6081211681512076 | RMSE 0.12483664602041245\n",
      "w=0.24 | Pearson 0.6114347049896172 | RMSE 0.1244916245341301\n",
      "w=0.26 | Pearson 0.6146539200027072 | RMSE 0.12415430694818497\n",
      "w=0.28 | Pearson 0.6177773802054599 | RMSE 0.12382475286722183\n",
      "w=0.3 | Pearson 0.6208036878837648 | RMSE 0.12350303679704666\n",
      "w=0.32 | Pearson 0.623731620860007 | RMSE 0.12318921834230423\n",
      "w=0.34 | Pearson 0.6265600713314836 | RMSE 0.12288334965705872\n",
      "w=0.36 | Pearson 0.6292881833754689 | RMSE 0.1225854828953743\n",
      "w=0.38 | Pearson 0.6319150471145307 | RMSE 0.12229570001363754\n",
      "w=0.4 | Pearson 0.6344400822001859 | RMSE 0.12201403826475143\n",
      "w=0.42 | Pearson 0.6368627609755023 | RMSE 0.12174057215452194\n",
      "w=0.44 | Pearson 0.6391826960283991 | RMSE 0.12147533148527145\n",
      "w=0.46 | Pearson 0.6413996680141871 | RMSE 0.12121839076280594\n",
      "w=0.48 | Pearson 0.6435136004776525 | RMSE 0.12096980959177017\n",
      "w=0.5 | Pearson 0.6455245596903486 | RMSE 0.12072962522506714\n",
      "w=0.52 | Pearson 0.6474327146434713 | RMSE 0.12049787491559982\n",
      "w=0.54 | Pearson 0.6492383939140147 | RMSE 0.1202746257185936\n",
      "w=0.56 | Pearson 0.6509420629172714 | RMSE 0.12005993723869324\n",
      "w=0.58 | Pearson 0.6525442992660819 | RMSE 0.11985383182764053\n",
      "w=0.6 | Pearson 0.6540458106288107 | RMSE 0.11965635418891907\n",
      "w=0.62 | Pearson 0.6554474137182278 | RMSE 0.11946757137775421\n",
      "w=0.64 | Pearson 0.6567500347345152 | RMSE 0.11928749829530716\n",
      "w=0.66 | Pearson 0.6579547614644292 | RMSE 0.11911618709564209\n",
      "w=0.68 | Pearson 0.6590626995927891 | RMSE 0.11895367503166199\n",
      "w=0.7000000000000001 | Pearson 0.6600751223505149 | RMSE 0.11879999190568924\n",
      "w=0.72 | Pearson 0.660993348693423 | RMSE 0.11865517497062683\n",
      "w=0.74 | Pearson 0.6618187947093848 | RMSE 0.11851926147937775\n",
      "w=0.76 | Pearson 0.6625530082144304 | RMSE 0.11839228123426437\n",
      "w=0.78 | Pearson 0.6631975396965858 | RMSE 0.1182742640376091\n",
      "w=0.8 | Pearson 0.6637540288132946 | RMSE 0.11816522479057312\n",
      "w=0.8200000000000001 | Pearson 0.6642242090186816 | RMSE 0.11806520074605942\n",
      "w=0.84 | Pearson 0.6646098601487898 | RMSE 0.11797421425580978\n",
      "w=0.86 | Pearson 0.6649127882473975 | RMSE 0.11789227277040482\n",
      "w=0.88 | Pearson 0.6651348408488555 | RMSE 0.1178194135427475\n",
      "w=0.9 | Pearson 0.6652779353676471 | RMSE 0.11775564402341843\n",
      "w=0.92 | Pearson 0.6653440119140965 | RMSE 0.1177009791135788\n",
      "w=0.9400000000000001 | Pearson 0.6653350356726857 | RMSE 0.1176554411649704\n",
      "w=0.96 | Pearson 0.6652530152453251 | RMSE 0.11761902272701263\n",
      "w=0.98 | Pearson 0.6650999501887319 | RMSE 0.1175917387008667\n",
      "w=1.0 | Pearson 0.6648778360513673 | RMSE 0.11757360398769379\n",
      "\n",
      ">>> best weight (max Pearson)\n",
      "w = 0.92 → Pearson 0.6653440119140965 | RMSE 0.1177009791135788\n",
      "\n",
      "training meta-learner …\n",
      "Stack Pearson 0.6653493408926945 | RMSE 0.11684946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/meta_model.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, joblib, pathlib\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "RNG = 42\n",
    "N_FOLDS = 10\n",
    "pathlib.Path(\"predictions\").mkdir(exist_ok=True)\n",
    "\n",
    "# load feature matrices\n",
    "# UMich train features\n",
    "df_um_tr = pd.read_csv(\"./output/umich_train_features_clean.csv\")\n",
    "X_um_tr = df_um_tr.drop(columns=[\"Dataset\",\"Mixture 1\",\"Mixture 2\", \"Experimental Values\"]).values\n",
    "\n",
    "# Belfaction features\n",
    "X_bf_tr    = np.load(\"./output/belfaction_pair_train_X.npy\")\n",
    "\n",
    "\n",
    "# load target values\n",
    "y_train = np.load(\"./output/belfaction_pair_train_y.npy\").astype(np.float32)\n",
    "\n",
    "print(f\"UMich train shape : {X_um_tr.shape}\")\n",
    "print(f\"Belfact train shape : {X_bf_tr.shape}\")\n",
    "\n",
    "# build base model\n",
    "# UMich CatBoost\n",
    "cat_params = dict(\n",
    "    iterations = 400,\n",
    "    depth = 6,\n",
    "    learning_rate = 0.03,\n",
    "    l2_leaf_reg = 3,\n",
    "    loss_function = \"RMSE\",\n",
    "    random_seed = RNG,\n",
    "    verbose = 0\n",
    ")\n",
    "\n",
    "def fresh_umich():\n",
    "    return CatBoostRegressor(**cat_params)\n",
    "\n",
    "# Belfaction ExtraTrees\n",
    "def fresh_belf():\n",
    "    return make_pipeline(\n",
    "        SimpleImputer(strategy=\"mean\"),\n",
    "        ExtraTreesRegressor(\n",
    "            n_estimators = 1000,\n",
    "            random_state = RNG,\n",
    "            n_jobs       = -1\n",
    "        )\n",
    "    )\n",
    "\n",
    "# OOF predictions for both base models\n",
    "print(\"\\ncomputing OOF predictions\")\n",
    "\n",
    "kf = KFold(N_FOLDS, shuffle=True, random_state=RNG)\n",
    "oof_umich = np.empty_like(y_train, dtype=np.float32)\n",
    "oof_belf = np.empty_like(y_train, dtype=np.float32)\n",
    "\n",
    "for fold,(tr,vl) in enumerate(kf.split(X_um_tr),1):\n",
    "    um = fresh_umich()\n",
    "    um.fit(X_um_tr[tr], y_train[tr])\n",
    "    oof_umich[vl] = um.predict(X_um_tr[vl])\n",
    "\n",
    "    bf = fresh_belf()\n",
    "    bf.fit(X_bf_tr[tr], y_train[tr])\n",
    "    oof_belf[vl] = bf.predict(X_bf_tr[vl])\n",
    "\n",
    "    print(f\"fold {fold:2d} done\")\n",
    "\n",
    "np.save(\"predictions/oof_umich.npy\", oof_umich)\n",
    "np.save(\"predictions/oof_belf.npy\", oof_belf )\n",
    "\n",
    "# simple 50-50 ensemble\n",
    "simple_blend = 0.5*oof_umich + 0.5*oof_belf\n",
    "print(\"\\nSimple 50-50 ensemble\")\n",
    "print(\"Pearson =\", pearsonr(y_train, simple_blend)[0],\n",
    "      \"|  RMSE\", np.sqrt(((y_train-simple_blend)**2).mean()))\n",
    "\n",
    "# optimal scalar weight ensemble\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nsearching best scalar weight …\")\n",
    "\n",
    "best_w, best_p, best_rmse = None, -9.0, 99.0\n",
    "\n",
    "for w in np.linspace(0, 1, 51):\n",
    "    z = (1 - w) * oof_umich + w * oof_belf\n",
    "    pear = pearsonr(y_train, z)[0]\n",
    "    rmse = np.sqrt(((y_train - z) ** 2).mean())\n",
    "\n",
    "    if pear > best_p:\n",
    "        best_w, best_p, best_rmse = w, pear, rmse\n",
    "\n",
    "    print(f\"w={w} | Pearson {pear} | RMSE {rmse}\")\n",
    "\n",
    "print(\"\\n>>> best weight (max Pearson)\")\n",
    "print(f\"w = {best_w} → Pearson {best_p} | RMSE {best_rmse}\")\n",
    "\n",
    "# Stacking meta-learner\n",
    "print(\"\\ntraining meta-learner …\")\n",
    "X_meta = np.column_stack([oof_umich, oof_belf])\n",
    "alphas = np.logspace(-4,4,25)\n",
    "\n",
    "meta = LinearRegression(fit_intercept=True, positive=True)\n",
    "meta.fit(X_meta, y_train)\n",
    "meta_pred  = meta.predict(X_meta)\n",
    "print(\"Stack Pearson\", pearsonr(y_train, meta_pred)[0],\n",
    "      \"| RMSE\", np.sqrt(((y_train-meta_pred)**2).mean()))\n",
    "joblib.dump(meta, \"models/meta_model.joblib\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee403e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I did not use the leaderboard and test sets in the end, \n",
    "## but you can uncomment this section to use them if needed.\n",
    "\n",
    "#X_um_lead = pd.read_csv(\"./umich_leaderboard_features.csv\")\\\n",
    "                #.drop(columns=[\"Dataset\",\"Mixture 1\",\"Mixture 2\"]).values\n",
    "#X_um_test = pd.read_csv(\"./umich_test_features.csv\")\\\n",
    "                #.drop(columns=[\"Dataset\",\"Mixture 1\",\"Mixture 2\"]).values\n",
    "\n",
    "#X_bf_lead  = np.load(\"./data/belfaction_pair_lead_X.npy\")\n",
    "#X_bf_test  = np.load(\"./data/belfaction_pair_test_X.npy\")\n",
    "\n",
    "\n",
    "## fit base models on all data & emit leaderboard / test\n",
    "\n",
    "#cat_full = fresh_umich().fit(X_um_tr, y_train)\n",
    "#cat_full.save_model(\"models/umich_catboost_final.cbm\")\n",
    "#joblib.dump(cat_full, \"models/umich_catboost_final.joblib\")\n",
    "\n",
    "#belf_full = fresh_belf().fit(X_bf_tr, y_train)\n",
    "#joblib.dump(belf_full, \"models/belf_extratrees_final.joblib\")\n",
    "\n",
    "## predictions\n",
    "#def emit(name, preds):\n",
    "    #np.save(f\"predictions/{name}.npy\", preds)\n",
    "\n",
    "## Leaderboard\n",
    "#p_um_lead  = cat_full .predict(X_um_lead)\n",
    "#p_bf_lead  = belf_full.predict(X_bf_lead)\n",
    "\n",
    "#emit(\"lead_simple50\", 0.5*p_um_lead + 0.5*p_bf_lead)\n",
    "#emit(\"lead_optw\"    , (1-best_w)*p_um_lead + best_w*p_bf_lead)\n",
    "#emit(\"lead_stack\"   , meta.predict(np.column_stack([p_um_lead, p_bf_lead])))\n",
    "\n",
    "## Test\n",
    "#p_um_test = cat_full .predict(X_um_test)\n",
    "#p_bf_test = belf_full.predict(X_bf_test)\n",
    "\n",
    "#emit(\"test_simple50\", 0.5*p_um_test + 0.5*p_bf_test)\n",
    "#emit(\"test_optw\"    , (1-best_w)*p_um_test + best_w*p_bf_test)\n",
    "#emit(\"test_stack\"   , meta.predict(np.column_stack([p_um_test, p_bf_test])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
